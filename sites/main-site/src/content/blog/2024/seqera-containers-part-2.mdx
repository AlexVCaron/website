---
title: 'Migration from Biocontainers to Seqera Containers: Part 2'
subtitle: How this will work and how we'll automate it
pubDate: 2024-09-24T09:20:00+01:00
headerImage: https://images.unsplash.com/photo-1711109631679-c9e1094d3118
headerImageAlt: Photo by Rafael Garcin on Unsplash
authors:
    - 'ewels'
    - 'edmundmiller'
label:
    - 'modules'
    - 'wave'
    - 'seqera containers'
embedHeaderImage: false
---

import creation_flow from '@assets/images/blog/seqera-containers-part-2/creation_flow.excalidraw.svg?raw';
import renovate_flow from '@assets/images/blog/seqera-containers-part-2/renovate_flow.excalidraw.svg?raw';

import { Image } from 'astro:assets';
import { YouTube } from '@astro-community/astro-embed-youtube';

# Introduction

[Wave](https://seqera.io/wave/) is a open-source software that Seqera released in the fall of 2022 in Barcelona.
It enables users to create software containers on demand, to rapidly speed up the development cycle.
Wave has a number of tricks up its sleeve, but in its simplest form, it can be passed a list of conda packages
and build a container with those packages installed.

[Seqera Containers](https://seqera.io/containers/) is a service for the bioinformatics community that's built on top of Wave,
providing an easy user interface to quickly create a container with a set of conda packages
and share that container image with others.

In nf-core we've been excited to better adopt Wave in our workflows, but have been looking for the right way to do it.
With the announcement of Seqera Containers we felt it was the right time to put in the effort to migrate
our containers to be built using Wave, using Seqera Containers to host the container images for our modules.

You can read more about our motivation for this change in [Part 1 of this blog post](https://nf-co.re/blog/2024/seqera-containers-part-1).
Here in Part 2 we will dig into the technical details: how it all works behind the curtain.
You don't need to know or understand any of this as an end-user of nf-core pipelines,
or even as a contributor to nf-core modules, but we thought it would be interesting to share the details.
It's mostly to serve as an architectural plan for the nf-core maintainers and infrastrcuture teams.

# The end goal

Before we dig into how the details of how the automation will work, let's summarise the end goal of this migration:

> "What we call the beginning is often the end. And to make an end is to make a beginning. The end is where to start from."
>
> _T.S. Eliot_

## Usage summary

The main thing to know is that in the future, maintainers will now _only_ need to change
module `environment.yml` files to update or change software dependencies.

Pipeline users will see almost no change in current behaviour, but have several new configuration profiles available:

| `-profile`             | Status    | Use case                                                                           |
| ---------------------- | --------- | ---------------------------------------------------------------------------------- |
| `docker`               | Unchanged | Docker images for `linux/amd64`                                                    |
| `singularity`          | Unchanged | Singularity images for `linux/amd64`                                               |
| `docker_arm`           | New       | Docker images for `linux/arm64`                                                    |
| `singularity_arm`      | New       | Singularity images for `linux/arm64`                                               |
| `singularity_oras`     | New       | Singularity images for `linux/amd64` using the `oras://` protocol                  |
| `singularity_oras_arm` | New       | Singularity images for `linux/arm64` using the `oras://` protocol                  |
| `apptainer`            | Updated   | Singularity images for `linux/amd64` (not Docker, as previously)                   |
| `apptainer_arm`        | New       | Singularity images for `linux/arm64`, using apptainer                              |
| `apptainer_oras`       | New       | Singularity images for `linux/amd64` using the `oras://` protocol, using apptainer |
| `apptainer_oras_arm`   | New       | Singularity images for `linux/arm64` using the `oras://` protocol, using apptainer |
| `conda`                | Updated   | Conda lockfiles for `linux/amd64`                                                  |
| `conda_arm`            | New       | Conda lockfiles for `linux/arm64`                                                  |
| `conda_local`          | New       | Conda environments with local environment resolution                               |
| `mamba`                | Updated   | Conda lockfiles for `linux/amd64`, using Mamba                                     |
| `mamba_arm`            | New       | Conda lockfiles for `linux/arm64`, using Mamba                                     |
| `mamba_local`          | New       | Conda environments with local environment resolution, using Mamba                  |

Glossary:

-   [`linux/amd64`](https://en.wikipedia.org/wiki/X86-64): Regular intel CPUs (aka `x86_64`)
-   [`linux/arm64`](https://en.wikipedia.org/wiki/AArch64): ARM CPUs (eg. AWS Graviton, aka `AArch64`). Not Apple Silicon.
-   [Apptainer](https://apptainer.org/): Alternative to Singularity, uses same image format
-   [Mamba](https://mamba.readthedocs.io): Alternative to Conda, uses same conda environment files

## Modules

All nf-core pipelines use a single container per process, and the majority of processes are
encapsulated within shared modules in the [nf-core/modules](https://github.com/nf-core/modules) repository.
As such, we must start with containers at the module level.

:::tip{.fa-brands.fa-github title="GitHub issue"}
For the latest discussion and progress on _bulk-updating_ existing nf-core modules, please see GitHub issue
[nf-core/modules#6698](https://github.com/nf-core/modules/issues/6698).
:::

### `main.nf`

With this switch, we simplify the `container` declaration, listing only the default container image: Docker, for `linux/amd64`.
There will no longer be any string interpolation or logic within the container string.

The `container` string is **never edited by hand** and is fully handled by the modules automation.

With [the FastQC module](https://github.com/nf-core/modules/blob/f768b283dbd8fc79d0d92b0f68665d7bed94cabc/modules/nf-core/fastqc/main.nf#L6-L8)
as an example:

```diff title="main.nf"
process FASTQC {
     label 'process_medium'

     conda "${moduleDir}/environment.yml"
+    container "fastqc:0.12.1--5cfd0f3cb6760c42" // automatically generated
-    container "${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?
-        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :
-        'biocontainers/fastqc:0.12.1--hdfd78af_0' }"

     input:
     tuple val(meta), path(reads)
```

We considered removing both `conda` and `container` declarations from the module `main.nf` file entirely,
however we see benefit for keeping these in this form because:

-   It's clearer to those exploring the code about what the module requires.
-   The container string is needed to tie the module to the pipeline config files
    (see [Building config files](#building-config-files) below).

Removing the container logic from the string should be a big win for readability.

### `meta.yml`

Through the magic of automation, we will append and then validate the following fields
within the module's `meta.yml` file. Following the
[FastQC example](https://github.com/nf-core/modules/blob/f768b283dbd8fc79d0d92b0f68665d7bed94cabc/modules/nf-core/fastqc/meta.yml)
from above:

```yaml title="meta.yml"
# ..existing meta.yml content above
containers:
    docker:
        linux_amd64:
            name: community.wave.seqera.io/library/fastqc:0.12.1--5cfd0f3cb6760c42
            build_id: 5cfd0f3cb6760c42_1
            scan_id: 6fc310277b74
        linux_arm64:
            name: community.wave.seqera.io/library/fastqc:0.12.1--d3caca66b4f3d3b0
            build_id: d3caca66b4f3d3b0_1
            scan_id: d9a1db848b9b
    singularity:
        linux_amd64:
            name: oras://community.wave.seqera.io/library/fastqc:0.12.1--0827550dd72a3745
            https: https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/b2/b280a35770a70ed67008c1d6b6db118409bc3adbb3a98edcd55991189e5116f6/data
            build_id: 0827550dd72a3745_1
        linux_arm64:
            name: oras://community.wave.seqera.io/library/fastqc:0.12.1--b2ccdee5305e5859
            https: https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/76/76e744b425a6b4c7eb8f12e03fa15daf7054de36557d2f0c4eb53ad952f9b0e3/data
            build_id: b2ccdee5305e5859_1
    conda:
        linux_amd64:
            lockfile: https://wave.seqera.io/v1alpha1/builds/5cfd0f3cb6760c42_1/condalock
        linux_arm64:
            lockfile: https://wave.seqera.io/v1alpha1/builds/d3caca66b4f3d3b0_1/condalock
```

The four different container images are all built at the same time so share the same snapshot of a given Conda resolution.
The build and scan IDs allow us to trace back to the build logs and security scans for these images.

The Conda lockfiles are a new addition to the nf-core ecosystem and will help reproducibility for Conda users.

:::info{.fa-lock title="Conda lockfiles recap" collapse}

Conda lockfiles were mentioned in [Part I of this blog post](/blog/2024/seqera-containers-part-1#exceptionally-reproducible).

> These pin the exact dependency stack used by the build, not just the top-level primary tool being requested.
> This effectively removes the need for conda to solve the build and also ships md5 hashes for every package.
> This will greatly improve the reproducibility of the software environments for conda users and the reliability of Conda CI tests.

Conda lockfiles are generated during the Docker image build and are specific to architecture.
The lockfiles can be accessed remotely via the Wave API, so we can treat them much in the same
way that we treat remote container images.

As a reminder, they look something like this:

```yaml title="FastQC Conda lockfile for linux/amd64"
# micromamba env export --explicit
# This file may be used to create an environment using:
# $ conda create --name <env> --file <this file>
# platform: linux-64
@EXPLICIT
https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2#d7c89558ba9fa0495403155b64376d81
https://conda.anaconda.org/conda-forge/linux-64/libgomp-13.2.0-h77fa898_7.conda#abf3fec87c2563697defa759dec3d639
https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_gnu.tar.bz2#73aaf86a425cc6e73fcf236a5a46396d
https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-13.2.0-h77fa898_7.conda#72ec1b1b04c4d15d4204ece1ecea5978
https://conda.anaconda.org/conda-forge/linux-64/alsa-lib-1.2.11-hd590300_1.conda#0bb492cca54017ea314b809b1ee3a176
https://conda.anaconda.org/conda-forge/linux-64/bzip2-1.0.8-hd590300_5.conda#69b8b6202a07720f448be700e300ccf4
# .. and so on
```

:::

## Pipelines

Containers are module-level are great, but we need to tie these into the pipelines where they will run.
Our guiding principles were to try to avoid changing current usage behaviour,
with full automation for pipeline maintainers.

The heart of the solution is that each container / conda environment will have a config file
auto-generated by the nf-core/tools CLI.
These files will not be edited by hand, so no manual merging will be required.
They'll simply be regenerated and overwritten every time a module is changed.

The config files will specify the `container` or `conda` directive for every process in the pipeline:

```groovy title="config/containers_docker_amd64.config"
// AUTOGENERATED CONFIG FILE - DO NOT EDIT
process { withName: 'NF_PIPELINE:FASTQC'         { container = 'fastqc:0.12.1--5cfd0f3cb6760c42' } }
process { withName: 'NF_PIPELINE:MULTIQC'        { container = 'multiqc:1.25--9968ff4994a2e2d7' } }
process { withName: 'NF_PIPELINE:ANALYSIS_PLOTS' { container = 'express_click_pandas_plotly_typing:58d94b8a8e79e144' } }
//.. and so on, for each process in the pipeline
```

Likewise, the conda config files will point to the lockfiles for each process:

```groovy title="config/conda_lockfiles_amd64.config"
// AUTOGENERATED CONFIG FILE - DO NOT EDIT
process { withName: 'NF_PIPELINE:FASTQC'         { conda = 'https://wave.seqera.io/v1alpha1/builds/5cfd0f3cb6760c42_1/condalock' } }
process { withName: 'NF_PIPELINE:MULTIQC'        { conda = 'https://wave.seqera.io/v1alpha1/builds/9968ff4994a2e2d7_1/condalock' } }
process { withName: 'NF_PIPELINE:ANALYSIS_PLOTS' { conda = 'https://wave.seqera.io/v1alpha1/builds/58d94b8a8e79e144_1/condalock' } }
//.. and so on, for each process in the pipeline
```

Singularity will have config files for both `oras` and `https` containers, so that users can choose which to use.

:::tip{.fa-comment-question title="Singularity: oras or https?" collapse}

Unfamiliar with `oras`? Don't worry, it's relatively new in the field.
It's a new protocol to reference container images, similar to `docker://` or `shub://`.
It allows Singularity to interact with any OCI ([Open Container Initiative](https://opencontainers.org/))
compliant registry to pull images.

Using `oras` has some advantages:

-   Singularity handles pulls in the process task, rather than it happening in the Nextflow head job
    -   This means less resource usage on the head node, and more parallelisation
-   Singularity can use authentication to pull from private registries
    (see [Singularity docs](https://docs.sylabs.io/guides/main/user-guide/cli/singularity_registry.html)).

However, there are some downsides:

-   Shared cache Nextflow options such as `$NXF_SINGULARITY_CACHEDIR` and `$NXF_SINGULARITY_LIBRARYDIR` do not work
-   Singularity must be installed to download images for offline use
-   It's not yet supported by all Singularity versions.

As such, we will continue to use `https` downloads for Singularity `SIF` images for now.
However, we will start to provide new `-profile singularity_oras` profiles for anyone who
would prefer to fetch images using the newer `oras` protocol.

If you'd like to know more, check out the amazing [bytesize talk](https://nf-co.re/events/2024/bytesize_singularity_containers_hpc)
by Marco Claudio De La Pierre ([@marcodelapierre](https://github.com/marcodelapierre/)) from June 2024:

<YouTube id="https://www.youtube.com/watch?v=zoCC_dkhjD0" poster="http://i3.ytimg.com/vi/zoCC_dkhjD0/hqdefault.jpg" />

:::

The main `nextflow.config` file will import these depending on the profile selected.

We're taking this opportunity to update the `apptainer` and `mamba` profiles too,
they will import the exact same config files as the `singularity` and `conda` profiles.

> Note: Boilerplate code removed from this code snippet for clarity.

:::note{.fa-code title="nextflow.config (collapsed as it's quite long)" collapse}

```groovy
// Set container for docker amd64 by default
includeConfig 'config/containers_docker_amd64.config'

profiles {
    docker {
        docker.enabled      = true // Use the default config/containers_docker_amd64.config
    }
    docker_arm {
        includeConfig       'config/containers_docker_linux_arm64.config'
        docker.enabled      = true
    }
    singularity {
        includeConfig       'config/containers_singularity_linux_amd64.config'
        singularity.enabled = true
    }
    singularity_arm {
        includeConfig       'config/containers_singularity_linux_arm64.config'
        singularity.enabled = true
    }
    singularity_oras {
        includeConfig       'config/containers_singularity_oras_linux_amd64.config'
        singularity.enabled = true
    }
    singularity_oras_arm {
        includeConfig       'config/containers_singularity_oras_linux_arm64.config'
        singularity.enabled = true
    }
    apptainer {
        includeConfig       'config/containers_singularity_linux_amd64.config'
        apptainer.enabled = true
    }
    apptainer_arm {
        includeConfig       'config/containers_singularity_linux_arm64.config'
        apptainer.enabled = true
    }
    apptainer_oras {
        includeConfig       'config/containers_singularity_oras_linux_amd64.config'
        apptainer.enabled = true
    }
    apptainer_oras_arm {
        includeConfig       'config/containers_singularity_oras_linux_arm64.config'
        apptainer.enabled = true
    }
    conda {
        includeConfig       'config/conda_lockfiles_amd64.config'
        conda.enabled       = true
    }
    conda_arm {
        includeConfig       'config/conda_lockfiles_arm64.config'
        conda.enabled       = true
    }
    conda_local {
        conda.enabled       = true // Use the environment.yml file in the module main.nf
    }
    mamba {
        includeConfig       'config/conda_lockfiles_amd64.config'
        conda.enabled       = true
        conda.useMamba      = true
    }
    mamba_arm {
        includeConfig       'config/conda_lockfiles_arm64.config'
        conda.enabled       = true
        conda.useMamba      = true
    }
    mamba_local {
        conda.enabled       = true // Use the environment.yml file in the module main.nf
        conda.useMamba      = true
    }
}

docker.registry      = 'community.wave.seqera.io/library'
podman.registry      = 'community.wave.seqera.io/library'
apptainer.registry   = 'oras://community.wave.seqera.io/library'
singularity.registry = 'oras://community.wave.seqera.io/library'
```

:::

Note that there are a few changes here:

-   New profiles with `_arm` suffixes for `linux/arm64` architectures
-   New profiles for `_oras` suffixes for using the `oras://` protocol
-   The `apptainer` profiles now uses the `singularity` config files
-   The `conda` profiles now use Conda lockfiles instead of `environment.yml` files
-   New `conda_local` profiles for those wanting to keep the old behaviour
-   New `mamba` profiles, using the `conda` config files
-   Base registry set to Seqera Containers

Because we're setting the base registry still, it should still be simple
to mirror containers to custom Docker registries and overwrite only
`docker.registry` as before.

# Automation - Modules

On to the good stuff - the promised land of automation!
Let's talk about how we're going to build all of these things without manual intervention.

:::tip{.fa-brands.fa-github title="GitHub issue"}
For the latest updates on modules container automation, see
[nf-core/modules#6694](https://github.com/nf-core/modules/issues/6694).
:::

## Updating conda packages

The automation begins when a contributor wants to add a piece of software to a container.
For instance, they decided that they need samtools installed.
The contributor updates the `environment.yml` and adds a line with samtools:

```yml title="environment.yml" {6}
channels:
    - conda-forge
    - bioconda
dependencies:
    - bioconda::fastqc=0.12.1
    - bioconda::samtools=1.16.1
```

That will kick off the container image generation factory
(it could equally be a change to remove a package, or change a pinned version).

A commit will be pushed automatically with an updated `meta.yml` file pointing to the new containers,
plus new nf-test snapshots for the software version checks.

Only the interesting part needs to be edited by the developer (which tools to use) and all other
steps are fully automated.

## Container image creation

As with most automation in nf-core, container creation will happen in GitHub Actions.
Edits to a module's `environment.yml` file will trigger a workflow that uses the
[`wave-cli`](https://github.com/seqeralabs/wave-cli) to build the container images.

1. GitHub Actions identifies changes in the `environment.yml` file.
2. `wave-cli` is executed on the updated environment file.
3. Seqera Containers builds new containers for various platforms and architectures.
4. GitHub Actions runs stub tests commits the updated the
   [version snapshot](https://github.com/nf-core/modules/blob/1fe2e6de89778971df83632f16f388cf845836a9/modules/nf-core/bowtie/align/tests/main.nf.test.snap#L32-L46).

<div class="excalidraw">
    <Fragment set:html={creation_flow} alt="Container creation flow" />
</div>

Note that we update the nf-test snapshot only for the stub tests, which
should only vary in the software versions.

Once this GitHub Actions run completes it will push the new commits back to the PR
and the regular nf-test CI will run.

If any output changes in the full nf-test CI, that test will fail and be obvious to maintainers.

## nf-test versions snapshot

One of the primary reasons that we were so excited to adopt nf-test was the snapshot in functionality.
Every test has a snapshot file with the expected outputs, and the hashes if the outputs are deterministic
(not a binary file, and there's no dates).

In that snapshot, we also capture the versions of the dependencies for the module
([example shown for bowtie2](https://github.com/nf-core/modules/blob/1fe2e6de89778971df83632f16f388cf845836a9/modules/nf-core/bowtie/align/tests/main.nf.test.snap#L32-L46)):

```json title="main.nf.test.snap" {4-7}
    "versions": {
        "content": [
            {
                "BOWTIE_ALIGN": {
                    "bowtie": "1.3.0",
                    "samtools": "1.16.1"
                }
            }
        ],
        "meta": {
            "nf-test": "0.9.0",
            "nextflow": "24.04.4"
        },
        "timestamp": "2024-09-27T10:42:58.892298"
    },
```

This gives a second level of confirmation that the containers were correctly generated.
We can automatically commit the updated snapshot file in the PR, so that
the developer doesn't need take an extra step.

## nf-core/modules - Renovate version bumps

The nf-core community loves automation. It's baked into the core of our community from our shared interest in automating workflows. We have linting bots, template updates, slack workflows, pipeline announcements. You name it, we've automated it.
We've recently adopted Renovate, a tool for automated dependency updates. It's multi-platform and multi-language. It's become pretty popular in the devops space. It's similar to [GitHub's dependabot](https://docs.github.com/en/code-security/getting-started/dependabot-quickstart-guide#about-dependabot), but supports more languages and frameworks, and more importantly for nf-core, enables us to write our own custom dependencies.
It runs on a schedule and automatically updates versions for us based on the specifications we've laid out in a common config: [https://github.com/nf-core/ops/blob/main/.github/renovate/default.json5](https://github.com/nf-core/ops/blob/main/.github/renovate/default.json5)

The magic starts with some nf-core automation to add renovate comments to the `environment.yml` file:

```yml {5,7,9}
channels:
    - conda-forge
    - bioconda
dependencies:
    # renovate: datasource=conda depName=bioconda/bwa
    - bioconda::bwa=0.7.18
    # renovate: datasource=conda depName=bioconda/samtools
    - bioconda::samtools=1.20
    # renovate: datasource=conda depName=bioconda/htslib
    - bioconda::htslib=1.20.0
```

[These comments will be added](https://github.com/nf-core/modules/issues/6504) through
[the batch module updates](https://github.com/nf-core/modules/issues/5828) happening this summer.

The renovate comments allow some scary regex to find the conda dependencies and their versions
in nf-core/modules and check if there's a new version available.

If there is a new version available, the Renovate bot will create a PR bumping the version,
which will kicks off the container creation GitHub Action..

When renovate bumps a dependency, the tests will fail because the version in the snapshot will not be up to date.
So we'll first bump just the version snapshot and then run all of the tests.

<div class="excalidraw">
    <Fragment set:html={renovate_flow} alt="Container creation flow" />
</div>

If all tests pass, the pull request is automatically merged without human intervention.
In case of test failures, Renovate automatically requests a review from the appropriate
module maintainer using the Codeowners file.
The maintainer then steps in to fix failing tests and request a final review before merging.

This efficient process ensures that software dependencies stay current with minimal manual oversight,
reducing noise and streamlining development workflows.

This will hopefully be the end of the _"can I get a review on this version bump"_ requests in `#review-requests`!

# Automation - Pipelines

## Building config files

Installing, updating or removing a module will trigger automation via the `nf-core` CLI
to regenerate all config files for the pipelines. Values will be taken from module `meta.yml` files where possible.

The first config file will be generated using the `nextflow inspect` command, generating the fully-qualified
process selectors for each process in the pipeline along with the default docker linux/amd64 container
that is defined in the module's `main.nf`.

After this file is created, it will be copied for Singularity, Conda and the other architectures.
nf-core/tools automation will search the `meta.yml` files for an exact match on the docker linux/amd64 container name
and replace this with the relevant container.
This way, we are able to link the shared module `meta.yml` container information with the fully qualified
process selectors, which depend on how the shared module is included into the pipeline code.

## Downloads

We will rewrite the `nf-core` CLI download tool to gather relevant container names from the module `meta.yml` files.
The code will then run `nextflow inspect . -concretize` on the pipeline to try to pick up any custom container declarations.

# Edge cases

We will still support custom `container` declarations in modules, for cases where it's
not possible to use Seqera Containers.

# Roadmap
