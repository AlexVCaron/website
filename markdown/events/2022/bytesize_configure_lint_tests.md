---
title: 'Bytesize: Configuring lint tests'
subtitle: Phil Ewels, Seqera labs
type: talk
start_date: '2022-11-08'
start_time: '13:00 CET'
end_date: '2022-11-08'
end_time: '13:30 CET'
location_url:
  - https://doi.org/10.6084/m9.figshare.21533151.v1
  - https://www.youtube.com/watch?v=DiXh3Dvpq5E
---

# nf-core/bytesize

Join us for our **weekly series** of short talks: **“nf-core/bytesize”**.

Just **15 minutes** + questions, we will be focussing on topics about using and developing nf-core pipelines.
These will be recorded and made available at <https://nf-co.re>
It is our hope that these talks / videos will build an archive of training material that can complement our documentation. Got an idea for a talk? Let us know on the [`#bytesize`](https://nfcore.slack.com/channels/bytesize) Slack channel!

## Bytesize: Configuring lint tests

This week, Phil Ewels ([@ewels](https://github.com/ewels)) will show how to configure lint tests.

<details markdown="1"><summary>Video transcription</summary>
**Note: The content has been edited for reader-friendliness**

[0:01](https://www.youtube.com/watch?v=DiXh3Dvpq5E&t=1)
Thank you. Hello, everyone. Welcome to today's Bitesites talk. My name is Francisca Bohnert. I'm today's host. But before we go into the talk, I would like to highlight an event. As you all know, we had a hackathon not long ago, which was hugely successful. And good news, there will be another one coming up in March 2023. And this one will be a completely virtual event, but we are planning to have local communities come together and join forces. So if you want to host one of those, please contact us or at a pull request to the website where you can add your own site. So, this is enough of announcements. Now I would like to hand over to Phil, who's giving today's talk. And he is talking about configuring linting. Right? Yes. Yeah, so this is me stepping in, doing a bit of a last minute talk again. So sorry for the late advertisement about this one, and about what the talk topic would be. We realized, actually a fan had been telling us for a couple of weeks that we were running out of talks, but we only really realized yesterday that we didn't have anything scheduled for today. So we had a look through a list of suggestions, and this was one which had been asked or requested a couple of times. So I thought I'd jump in and try and talk everyone through NF-Core linting, what it is, how it works, but most importantly, how to configure it. And this is going to be particularly interesting for both people building NF-Core pipelines, but also people working with NF-Core tooling outside of the NF-Core ecosystem, where maybe some of these tests are not really relevant to you. But it's good for everyone to understand how it works and how to configure it. So, in the spirit of last minute decisions, I'm going to be live demoing everything. So my apologies in advance if everything breaks horribly, but let's see how it goes. Okay, hopefully you can all see my screen. Move your faces over here. Wonderful. So, first things first, just a reminder about where the documentation is for all of this. So this is the NF-Core website. And if you head on over to the tools page up at the top here, you find the section for NF-Core lint. And this is the documentation. And basically, it pretty much tells you everything I'm going to tell you. So if you get distracted by a dog or a neighbour or something, or if you forget everything else I'm going to tell you about, then just remember this. There is documentation on the website and you can read it and it explains how everything works. With that said, let's jump in and do some demo and walk through what we're going to be talking about today. So just before we started recording, I created a new pipeline. And so I just did NF-Core create, which made me a new empty pipeline. And then I updated the modules and I cleared out some of the template stuff so that we're kind of up and running with a clean pipeline. And if I do git log, you can see I've just got two commits from when we started preparation for this talk and just resolving the link warnings. Now, what is code linting or linter? Basically, a code linter has a set of rules about how code should look and how it should work. And it checks the code and gives you passes or failures or in a series of tests. Typically, when you talk about code linters, you're also talking about code formatting. So, for example, you can have a JavaScript or JSON code formatter or Python code formatter. And these have linting tests where they look at the code and they can also reformat the code for you. We're not talking about that today. Then, of course, linter doesn't do anything to do with code formatting. It's just a set of rules and it's a way to inspect code for standardization. The linting has been around for NF-Core since we started the project and is a fairly crucial part of how we work. And it allows the community to scale because we do so much work with code review where other people are reviewing code that you're writing. Having a set of automated tests means that we can be a bit more confident that things are adhering to the principles and not breaking stuff without having to check really, really carefully. And so by automating the things that we are commonly asking for, it streamlines this process of code review and makes the general code quality better. And this is pretty much how the NF-Core lint tools have evolved over time. It started off with a couple of minor things. And every time we've come across the same thing a few times in a pull request, we say we should write a lint test for that so we can automatically test for this thing again in the future. Another thing that's really important about it is it makes sure that everybody stays up to date with all the latest things, guidelines and rules for NF-Core because these change over time. We have updates in the NF-Core template and we do template synchronization and all this stuff and that rolls along. And what happens is if you keep running NF-Core lints on your code base over time, as retooling updates along with the template, you'll start to get failures before it's passing. And that's because it says it checks the code in your pipeline against the template. And in some places it says this should be the same as the template. And it's either out of date or you've edited it and that's a bad thing. So it also kind of forces everyone to stay up to date and in sync. So that's why we have retooling. That's why we have NF-Core linting. And if I do NF-Core help, you'll see that it's one of the commands here, commands for developers. And the basic one is NF-Core lint. You can also call it just specifically for modules. So it's an NF-Core modules lint and that calls just a subset of those tests. But generally speaking, you just do NF-Core lint and it just checks everything. I'm currently sat in my working directory as the root of the pipeline. And here it's run one hundred and eighty two different tests and they all passed and nothing failed. And that's great. When I push commits and open pull requests to GitHub and things, we have automated continuous integration tests, zero tests, and they run the same command on the code. And so that means when you open a pull request, those tests automatically run and you get a green tick or red cross saying whether this is good or bad. We have different statuses that the tests can have so they can be passed, they can fail, which obviously is bad, and they can throw warnings. With warnings, they don't actually fail the CI, so you'll still get a green tick. But it tells you in the text message when you check the text that there's something which is maybe not ideal. And hopefully when you're creating pull requests, you also get a little comment automatically added, which summarizes the results in the body of the pull request. So to give you visibility, especially for these warnings, which might kind of fly under the radar otherwise. And what we're particularly interested about today is this category of tests, which are ignored. I'm going to come on and tell you how to ignore CI tests. But first, let's make something fail. So if I open up VS Code here, this is my pipeline that I've just created. And I'm going to dig in as different, lots of different tests which I could make fail. But I'm going to start off by doing something very simple. We've got the readme file here. And one of the tests checks that the nfcore readmes look like nfcore readmes. And all nfcore readmes have these badges at the top. I'll show you what I mean if I go to RNA-Seq. Go to github.com and look at the main readme on kind of when you load up the repo. It's got these badges along the top, which says it works with Nextflow and you need this version or whatever. So we have a lint test that checks these badges are there and correctly formatted and consistent. So, for example, it says the minimum version of Nextflow you need. You also define that in the config file down here. And it's quite easy to let these two fall out of sync. And so we have a lint test which checks this button in the readme. Simple. So I'm going to break it by deleting those. It's gone. Markdown file is still totally valid. It just doesn't have any of the buttons at the top. OK, so proof of principle, nfcore lint now should hopefully fail. OK, it didn't fail. It gave us a warning. Close enough. So this now really important about this is when you see these warnings, like you get a summary text saying didn't have a minimum Nextflow minimum version badge. But there might be it might be that you don't completely understand what the message is or it's a bit unclear. Most browsers will handle these the hyperlinks in the terminal and where it says readme, that's the identifier for this test. And if I hold, I'm on a Mac, so if I hold command, I think on Linux and Windows, it's control. I can click that and it's going to open up a tab in my web browser and it's going to go specifically to the test ID called readme. And then this is where we have longer form documentation about this specific lint test. And here you can see it says it needs to have a Nextflow badge and it should look like this and it should be the right thing. And it should have a bioconda badge and everything. So this is where the long form documentation about these lint tests are. You can also find it if you go to tools and then somewhere under tools. I always forget. Anyway, you follow that link here and it tells you all about it. OK, so I noticed that this is quite specific, it's not just editing the whole file, but it's just this part of the readme file is checked and there are other ones as well. So if I go into, let me see, assets, multiqc.config, do through bar, then I should get, I think, a failure about editing the multiqc file. And no. OK, I managed to do something right there. But anyway, there's certain files where you can get a failure for putting stuff at the start, but if you stick it at the end, it will be valid and things. And that test is, I think, called files unchanged here. So I should have picked one which is actually tested. So these ones, you can add extra stuff and see the gitignore file, for example. So, yeah, these are all different lint tests and they're documented here. So obviously I could fix this readme file by putting the badges back in, rerunning a template, etc, or reading the documentation and seeing what's required. But in this case, maybe I don't want to do that. So maybe I'm building outside of nfcore and I don't want to have the nfcore badges at the top and I want to do my own thing. And that's fine. How do I go about doing that? So I need to ignore this test. And the way I do that is when you run nfcore create, you get a config file for nfcore, this one,.nfcore.yml, which is a hidden file. So depending on how you're looking at your files, you may need to show hidden files. In here, by default, we just have this. It just says this repository is a pipeline. This is to do with working with modules. But I can add a new key in here called lint. And under lint, I'm going to give the name of the test that failed, which is called readme. And I'm going to set it to false. And I'm going to hit save. That's what it is. I'm going to rerun linting now. And now it says pipeline test ignored. And it just says it didn't run this test. And so because of that, nothing failed and everything's fine. That's basically all there is to it. If there are any of these lint tests you don't like that you are sure that it's doing what you want it to be doing, then you can just say ignore this test and it will be ignored. Some of these, so this is quite a blunt tool. I've just ignored this entire lint test called readme. And so, for example, if I go in and I change, let's do files exist here. So I'm going to delete the editor config file. And of course lint. Then it should throw a failure because this is a required file and it's not there. So it takes me here. Now I could do files exist false and that will disable the entire lint test. And that's fine. That makes everything work. But it's a bit of a blunt tool because now it's not checking for the presence of any files. So it's allowing me to delete that one file, but it's now not checking for the presence of any of these files, which is maybe kind of overkill because it was just that one file I care about. Now, some of the lint tests then allow you to provide a bit more information. So in this case, what was the file I deleted? Editor config. Instead of just saying false, I can actually give the name of the file there. I think it has to be a list. Now, when I run this again, hopefully it should still pass. But now it ignores just that one specific file. So this isn't possible for every single one of it. And of course, lint tests, you have to check the documentation. But certainly files exist and files unchanged, which are probably two of the ones which come up the most frequently. You can specify exactly which files you want to ignore and then you keep the value of the rest of the lint test there, which checks all the other stuff, which is generally kind of a good thing. Right. 13, 14. That's my 15 minutes. It's a very short and sharp bite sized talk this time. Very specific about this one kind of thing. But hopefully that's helpful. And hopefully this will be a useful resource for anyone coming back to this in the future, asking about how lint configs work. If you have any questions or feedback or suggestions, then please shout and I'll do my best to answer any questions if there are any now. I'm back to you. Yeah. Hi. So everyone can unmute themselves if they want to and just ask the question right away. I don't see anything coming up for now. I mean, you don't have to ask now. Obviously, as usual, you can come to Slack, either ask your questions in bite size or in the help. And as usual, I would like to thank the Jan Zuckerberg Initiative for funding the talks. And of course, Phil, for this very short notice talk today and the audience for listening. Also, just say on Slack that there's a Slack channel called Linting. I think it's called Linting, which is the place to go for any specific questions about Linting when you're generally confused. Yeah, I should have known there's always a Slack channel for everything. Thank you, Phil. Always.

</details>
